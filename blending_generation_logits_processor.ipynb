{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blending-generation-logits-processor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUa/tiNgFsJ6MJQp503oka"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome!\n",
        "This is a reference implementation of Plug-and-Blend (https://github.com/xxbidiao/plug-and-blend , which itself is based on https://arxiv.org/abs/2104.04039), using the LogitsProcessor framework new in Huggingface Transformers. Feel free to check them out if you are unclear of anything in this notebook.\n",
        "\n",
        "# Set things up\n",
        "Here we will download necessary model to set up the modifier network."
      ],
      "metadata": {
        "id": "o89gtWRUAMgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVxDrUWqACKB",
        "outputId": "bdfdb850-ad43-4868-ad15-939553571ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-28 20:36:48--  https://storage.googleapis.com/sfr-gedi-data/gedi_topic.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.128, 142.250.157.128, 142.251.8.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318630072 (1.2G) [application/zip]\n",
            "Saving to: ‘gedi_topic.zip’\n",
            "\n",
            "gedi_topic.zip      100%[===================>]   1.23G  50.0MB/s    in 30s     \n",
            "\n",
            "2022-02-28 20:37:19 (41.4 MB/s) - ‘gedi_topic.zip’ saved [1318630072/1318630072]\n",
            "\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Downloading the GeDi modifier model.\n",
        "!wget https://storage.googleapis.com/sfr-gedi-data/gedi_topic.zip\n",
        "import zipfile\n",
        "with zipfile.ZipFile('gedi_topic.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n",
        "\n",
        "gedi_path = \"gedi_topic/\"\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's set the Logits Processor up."
      ],
      "metadata": {
        "id": "dUBDHOehAJfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, LogitsProcessorList\n",
        "\n",
        "# Set CUDA device to cuda if gpu is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gedi_location = gedi_path\n",
        "\n",
        "class PlugAndBlendLogitsProcessor(transformers.LogitsProcessor):\n",
        "\n",
        "    gedi_model = GPT2LMHeadModel.from_pretrained(gedi_location).to(device)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # default omega from original GeDi work, higher disc_weight means more aggressive topic steering.\n",
        "    # can be overridden when calling generate_one_sentence(), see that function.\n",
        "    # default value (1x) is 30.\n",
        "    omega = 30\n",
        "\n",
        "    def __init__(self, topic: str, weight: float):\n",
        "        super().__init__()\n",
        "        self.topic = topic\n",
        "        self.weight = weight\n",
        "        self.encoded_topic = PlugAndBlendLogitsProcessor.tokenizer.encode(topic)[0]\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        #print(\"Applying topic: %s, weight: %s\" % (self.encoded_topic, self.weight))\n",
        "        # print(\"test %s\" % scores[:, 100])\n",
        "        # scores[:, 100] += 1\n",
        "        # print(\"after %s\" % scores[:, 100])\n",
        "        modifiers = self.get_gedi_modifiers(input_ids = input_ids)\n",
        "\n",
        "        # Make them appear on the same device\n",
        "        modifiers = modifiers.to(scores.device)\n",
        "\n",
        "        scores += modifiers * self.weight * PlugAndBlendLogitsProcessor.omega\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def get_gedi_modifiers(self, input_ids):\n",
        "\n",
        "        # Setting up some constants\n",
        "        code_0 = \"negative\"\n",
        "        code_1 = \"positive\"\n",
        "        nt_id = PlugAndBlendLogitsProcessor.tokenizer.encode(code_0)[0]\n",
        "        pt_id = PlugAndBlendLogitsProcessor.tokenizer.encode(code_1)[0]\n",
        "\n",
        "        # define class weights for cross entropy loss: give weight 0 to [50256], the padding (eot) token.\n",
        "        crossentropy_loss_weight = [1] * 50257\n",
        "        crossentropy_loss_weight[50256] = 0 # do not calculate loss on eos token\n",
        "        crossentropy_loss_weight = torch.tensor(crossentropy_loss_weight).float().to(device)\n",
        "\n",
        "        # Creating prefixes.\n",
        "        seq_pt = (torch.ones(input_ids.shape[0]) * pt_id).type_as(input_ids).view(-1, 1)\n",
        "        seq_nt = (torch.ones(input_ids.shape[0]) * nt_id).type_as(input_ids).view(-1, 1)\n",
        "        encoded_topic_torch = (torch.ones(input_ids.shape[0]) * self.encoded_topic).type_as(input_ids).view(-1, 1)\n",
        "\n",
        "        # Assemble input_ids.\n",
        "        seq_pt_new = torch.cat((seq_pt, encoded_topic_torch, input_ids), dim=1)[:, :]\n",
        "        seq_nt_new = torch.cat((seq_nt, encoded_topic_torch, input_ids), dim=1)[:, :]\n",
        "\n",
        "        def prepare_inputs_for_generation(input_ids, **kwargs):\n",
        "            return {\"input_ids\": input_ids.to(device)}\n",
        "\n",
        "        seq_batched = torch.cat([seq_pt_new,seq_nt_new], dim=0)\n",
        "\n",
        "        model_inputs = prepare_inputs_for_generation(input_ids=seq_batched)\n",
        "\n",
        "        gedi_outputs = PlugAndBlendLogitsProcessor.gedi_model(**model_inputs)\n",
        "\n",
        "        # Let's calculate modifier on the whole sentence:\n",
        "        # This is modifier on all tokens multiplied.\n",
        "        # Here, we calculate the baseline (sentence without generated token) modifier, for normalization.\n",
        "\n",
        "        shift_logits = gedi_outputs[\"logits\"][..., :-1, :].contiguous().to(device)\n",
        "        shift_labels = seq_batched[..., 1:].contiguous().to(device)\n",
        "\n",
        "        # By using Cross Entropy on previous tokens,\n",
        "        # This effectively picked probabilities of previous tokens in the sequence.\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\",\n",
        "                                             weight=crossentropy_loss_weight,\n",
        "                                             )\n",
        "\n",
        "        # Cross entropy loss originally gives -p(x), so...\n",
        "        logits_r = -1 * loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "        )\n",
        "        logits_r = logits_r.view(seq_batched.shape[0], -1)\n",
        "\n",
        "        seq_len = logits_r.shape[1]\n",
        "\n",
        "        logits_r = torch.sum(logits_r, 1)\n",
        "\n",
        "        # Now, finally add the baseline into the actual final (generated token) logits.\n",
        "        gedi_logits = torch.log_softmax(gedi_outputs[\"logits\"][:, -1, :], -1)\n",
        "        gedi_logits += logits_r.unsqueeze(1)\n",
        "\n",
        "        # Normalize modifier logits by sequence length and reshape it for output\n",
        "        gedi_logits_split = torch.split(gedi_logits / seq_len,\n",
        "                                        input_ids.shape[0])\n",
        "\n",
        "        logits = torch.stack(gedi_logits_split, 2)\n",
        "\n",
        "        logp_related_softmax = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "        # Once normalized, we only care about the \"positive\" dimension (0).\n",
        "        final_modifier = logp_related_softmax[...,0]\n",
        "\n",
        "        return final_modifier\n",
        "\n",
        "# Tests\n",
        "\n",
        "def test_generation(prompt = None, topics = None):\n",
        "    if prompt is None:\n",
        "      prompt = \"Once upon a time,\"\n",
        "    \n",
        "    if topics is None:\n",
        "      # default topics\n",
        "      topics = {\"Science\":1,\"Nature\":1}\n",
        "\n",
        "    \n",
        "\n",
        "    #print(transformers.__version__)\n",
        "\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Set up the base language model.\n",
        "    # As this is plug-and-blend, you can change this to any model that uses the GPT2 tokenizer (i.e. has the same input_ids => actual sentence mapping).\n",
        "    # We are using GPT-2 here just as an example.\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Default prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    #input_ids = torch.cat([input_ids,input_ids,input_ids],dim=0)\n",
        "\n",
        "    lp_raw_list = []\n",
        "    for item in topics:\n",
        "      lp_raw_list.append(PlugAndBlendLogitsProcessor(topic=item, weight=topics[item]))\n",
        "    #lp_raw_list = [PlugAndBlendLogitsProcessor(topic=\"Science\", weight=1), PlugAndBlendLogitsProcessor(topic=\"Nature\", weight=1)]\n",
        "\n",
        "    lp_list = LogitsProcessorList(lp_raw_list)\n",
        "\n",
        "    greedy_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        logits_processor=lp_list,\n",
        "        no_repeat_ngram_size=2,\n",
        "    )\n",
        "    print(\"Output:\\n\" + 100 * '-')\n",
        "    print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "    # greedy_output = model.generate(\n",
        "    #     input_ids,\n",
        "    #     max_length=50,\n",
        "    #     logits_processor=lp_list,\n",
        "    # )\n",
        "    # print(\"Output:\\n\" + 100 * '-')\n",
        "    # print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BruN48wAI2f",
        "outputId": "cdfc0a35-6384-4689-ece8-e0e3685ece99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at gedi_topic/ were not used when initializing GPT2LMHeadModel: ['logit_scale']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate things (Demo)\n",
        "\n",
        "This demo showcases generation using GPT-2 as base model. Refer to the content of this function to see how you can use a different model (as long as its tokenizer is `GPT2Tokenizer.from_pretrained(\"gpt2\")` . \n",
        "\n",
        "Change test_prompt for prompt; change topics dictionary for topics you want to include in the generated sentence. 1 (all weights added up) gives standard control strength, and in our experiments 2 to 4 gives stronger steering."
      ],
      "metadata": {
        "id": "KPuKp9iUDgJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_topics = {\"Business\":0.5, \"Science\":0.5}\n",
        "test_prompt = \"Once upon a time,\"\n",
        "\n",
        "test_generation(prompt=test_prompt, topics=test_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_9n-y3UAwQH",
        "outputId": "b1a51c89-a49e-4af4-f69c-096a174d4bcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
            "\n",
            "The world that was created was not the same\n"
          ]
        }
      ]
    }
  ]
}